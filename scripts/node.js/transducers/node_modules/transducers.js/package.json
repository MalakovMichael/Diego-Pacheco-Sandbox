{
  "name": "transducers.js",
  "version": "0.1.1",
  "author": {
    "name": "James Long",
    "email": "longster@gmail.com"
  },
  "main": "./transducers.js",
  "devDependencies": {
    "benchmark": "^1.0.0",
    "es6-macros": "0.0.7",
    "expect.js": "^0.3.1",
    "gulp": "^3.8.8",
    "gulp-header": "^1.1.1",
    "gulp-sourcemaps": "^1.1.5",
    "gulp-sweetjs": "^0.5.2",
    "immutable": "^2.0.17",
    "lodash": "^2.4.1",
    "mocha": "^1.21.4",
    "source-map-support": "^0.2.7",
    "sweet.js": "^0.7.1",
    "underscore": "^1.7.0",
    "webpack": "^1.4.0-beta9"
  },
  "readme": "\n# transducers.js\n\nA small library for generalizing transformation of data. This provides a small amount a transformation functions that can be applied to any data structure. It is a direct port of Clojure's [transducers](http://blog.cognitect.com/blog/2014/8/6/transducers-are-coming) in JavaScript. *This is early work and should not be used in production yet*\n\n```\nnpm install transducers.js\n```\n\nFor browsers, grab the file `dist/transducers.js`.\n\nWhen writing programs, we frequently write methods that take in collections, do something with them, and return a result. The problem is that we frequently only write these functions to work a specific data structure, so if we ever change our data type or wanted to reuse that functionality, you can't. We need to decouple these kinds of concerns.\n\nA transducer is just a reducing function that transforms the value in some way. A reducing function has the form `function(result, input) {}` and returns a new result. If we express our transformations as these functions, we can easily compose them together without any knowledge of the source or destination result. [Read the introduction blog post](#) for much more thorough explanation.\n\nAvailable transformations:\n\n* `map(f, coll?)`\n* `filter(f, coll?)`\n* `remove(f, coll?)`\n* `keep(f, coll?)`\n* `dedupe(coll?)`\n* `take(n, coll?)`\n* `takeWhile(f, coll?)`\n* `drop(n, coll?)`\n* `dropWhile(f, coll?)`\n* `cat`\n* `mapcat(f)`\n\nMost of these transformations optionally takes a collection, and it will immediately run the transformation over it. These are highly optimized for builtin data types so if you pass an array in `map` it literally just runs a `while` loop and calls your function on each value.\n\nIf you don't pass a collection, it returns a transducer that you can apply in several ways. You can use `compose` to combine transformations. Here's an example. `sequence` returns a collection of the same type with the transformations applied to each value:\n\n```js\nsequence(\n  compose(\n    cat,\n    map(x => x + 1),\n    dedupe(),\n    drop(3)\n  ),\n  [[1, 2], [3, 4], [4, 5]]\n)\n// -> [ 5, 6 ]\n```\n\n## Applying Transducers\n\nUse transducers with the following functions:\n\n* `sequence(xform, coll)` - get a collection of the same type and fill it with the results of applying `xform` over each item in `coll`\n* `transduce(xform, f, init, coll)` - reduce a collection starting with the initial value `init`, applying `xform` to each value and running the reducing function `f`\n* `into(to, xform, from)` - apply xform to each value in the collection `from` and append it to the collection `to`\n\nAdditionally, a CSP channel from [js-csp](https://github.com/jlongster/js-csp) can take a transducer: `chan(1, xform)`. You can apply the exact same transformations over channels (which are basically streams!):\n\n```js\nvar ch = chan(1, compose(\n  cat,\n  map(x => x + 1),\n  dedupe(),\n  drop(3)\n));\n\ngo(function*() {\n  yield put(ch, [1, 2]);\n  yield put(ch, [3, 4]);\n  yield put(ch, [4, 5]);\n});\n\ngo(function*() {\n  while(!ch.closed) {\n    console.log(yield take(ch));\n  }\n});\n\n// output: 5, 6\n```\n\n## Iterating and Building\n\nIn order to apply a transducer, we need to know two things: how to iterate the collection and how to build up a new collection (assuming we aren't using `transduce` where you can build up anything). Luckily ES6 already gives a protocol for iteration, so anything conforming to that can be iterated over (generators, NodeLists, etc). If you attempt to iterate over an object, transducers.js will automatically convert it into an array of two-dimensional arrays of key/value pairs.\n\nHere's just a few examples:\n\n```js\nvar xform = compose(map(x => x * 2),\n                    filter(x => x > 5));\n\ninto([], xform, [1, 2, 3, 4]);\n// -> [ 6, 8 ]\n\nconsole.log(into([],\n     map(kv => [kv[0], kv[1] * 2]),\n                 { x: 1, y: 2 }));\n// -> [ [ 'x', 2 ], [ 'y', 4 ] ]\n\nfunction *data() {\n  yield 1;\n  yield 2;\n  yield 3;\n  yield 4;\n}\n\ninto([], xform, data());\n// -> [ 6, 8 ]\n\n// assuming div.page and div.article exist:\ninto([], map(x => x.className), document.querySelectorAll('div'));\n// -> [ '.page', '.article' ]\n```\n\nIn all of those examples, we are collecting the results into an array. So what about building data structures? What if we want an object, or something else back?\n\nIf you ask to build up an object, transducers.js will automatically transform an array of two-dimensional array key/value pairs into an object. Unfortunately, there is no existing protocol to make this happen for arbitrary data types like there is for iteration. We have to make our own.\n\nIf you are authoring a new data structure, you could provide functions for running transducers and implement that yourself. But we don't really want to force authors to be aware of transducers, and it's healthier for the community if we adopt protocols instead. So I'm proposing two new methods that all data structures can implement: `@@append` and `@@empty`.\n\n* `@@append` - add a new item to the collection\n* `@@empty` - return a newly-allocated empty collection of the same type\n\nWith these two methods, we can build up new collections arbitrarily without caring our their actual implementation. They are prefixed with two `at`s because ideally they are ES6 symbols, and that's how we write them in docs. Since symbols aren't implemented everywhere yet, you can just add methods literally called `\"@@append\"`.\n\nWith JavaScript prototypes, you can actually monkeypatch existing libraries quite easily. Let's say we wanted to use [immutable-js](https://github.com/facebook/immutable-js). It already implement the iterator protocol with a method `@@iterator`, but let's add two more:\n\n```js\nImmutable.Vector.prototype['@@append'] = function(x) {\n  return this.push(x);\n};\n\nImmutable.Vector.prototype['@@empty'] = function(x) {\n  return Immutable.Vector();\n};\n```\n\nNow we can work with `Immutable.Vector` in all of our functions:\n\n```\ninto(Immutable.Vector(),\n     map(x => x + 1),\n     [1, 2, 3, 4]);\n// -> Immutable.Vector(2, 3, 4, 5)\n\nsequence(compose(\n           map(x => x * 2),\n           filter(x => x > 5)\n         ),\n         Immutable.Vector(1, 2, 3, 4));\n// -> Immutable.Vector(6, 8)\n```\n\nYou could do the same thing with ES6 `Set` and `Map` types. Separating concerns provides a powerful way to write programs that can be reused easily.\n\n[BSD LICENSE](#)\n",
  "readmeFilename": "README.md",
  "description": "A small library for generalizing transformation of data. This provides a small amount a transformation functions that can be applied to any data structure. It is a direct port of Clojure's [transducers](http://blog.cognitect.com/blog/2014/8/6/transducers-are-coming) in JavaScript. *This is early work and should not be used in production yet*",
  "_id": "transducers.js@0.1.1",
  "_shasum": "e9272076065fff5639ca7234d0ddb465068b0810",
  "_from": "transducers.js@",
  "_resolved": "https://registry.npmjs.org/transducers.js/-/transducers.js-0.1.1.tgz"
}
